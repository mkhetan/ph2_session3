{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EVA_P2S3_backup_feb15_10pm.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mkhetan/ph2_session3/blob/master/EVA_P2S3_backup_feb15_10pm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jofyc9OC4Qcf",
        "colab_type": "text"
      },
      "source": [
        "#Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahBVnrNc3E0U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "plt.style.use('seaborn-white')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crQSAaIz4SkA",
        "colab_type": "text"
      },
      "source": [
        "# Read and process data. \n",
        "\n",
        "Download the file from this URL: https://drive.google.com/file/d/1UWWIi-sz9g0x3LFvkIZjvK1r2ZaCqgGS/view?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERTyX0dGtRs-",
        "colab_type": "code",
        "outputId": "c82baa9a-5fb9-4cd6-fbbb-890c8d384cee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        }
      },
      "source": [
        "## Mount google drive \n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "%cd gdrive/My Drive/colab/eva_phase2/session3\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "/content/gdrive/My Drive/colab/eva_phase2/session3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOqn9WK1tfqv",
        "colab_type": "code",
        "outputId": "4a83be2a-620c-437c-cb74-4f626aac0828",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "!pwd\n",
        "!ls -l"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/colab/eva_phase2/session3\n",
            "total 11\n",
            "-r-------- 1 root root 10346 Jan 30 05:17 text.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgOGxPDP3Wpp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = open('text.txt', 'r').read()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeXXMLRb4kXb",
        "colab_type": "text"
      },
      "source": [
        "Process data and calculate indices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5TKeiOp4jtl",
        "colab_type": "code",
        "outputId": "979b2db0-163f-4c93-e20f-6e79b51aa691",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "chars = list(set(data))\n",
        "data_size, X_size = len(data), len(chars)\n",
        "print(\"Corona Virus article has %d characters, %d unique characters\" %(data_size, X_size))\n",
        "char_to_idx = {ch:i for i,ch in enumerate(chars)}\n",
        "idx_to_char = {i:ch for i,ch in enumerate(chars)}"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Corona Virus article has 10223 characters, 75 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4C53MB135LRY",
        "colab_type": "text"
      },
      "source": [
        "# Constants and Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfj21ORa49Ps",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Hidden_Layer_size = 100 #size of the hidden layer\n",
        "Time_steps = 40 # Number of time steps (length of the sequence) used for training\n",
        "learning_rate = 1e-1 # Learning Rate\n",
        "weight_sd = 0.1 #Standard deviation of weights for initialization\n",
        "z_size = Hidden_Layer_size + X_size #Size of concatenation(H, X) vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdmJf4Du5uhb",
        "colab_type": "text"
      },
      "source": [
        "# Activation Functions and Derivatives"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seGHei_D5FGk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import sympy as sym\n",
        "\n",
        "def truncate(number, digit) -> float:\n",
        "  stepper = 10**digit\n",
        "  intNum = int(number*stepper)\n",
        "  return (intNum/stepper)\n",
        "\n",
        "def sigmoid(x): # sigmoid function\n",
        "  # return ( 1/(1 + e^(-x)))\n",
        "  y = (1/(1 + np.exp(-x)))\n",
        "  #y = truncate(y, 1)\n",
        "  return y # write your code here\n",
        "\n",
        "def dsigmoid(y): # derivative of sigmoid function\n",
        "  f = (1/(1 + np.exp(-y)))\n",
        "  df = f*(1-f)\n",
        "  return df\n",
        "\n",
        "def tanh(x): # tanh function\n",
        "  #y = (2*sigmoid(2x) -1)\n",
        "  y = (np.exp(x) - np.exp(-x))/(np.exp(x) + np.exp(-x))\n",
        "  return y\n",
        "\n",
        "def dtanh(y): # derivative of tanh\n",
        "  f = (np.exp(y) - np.exp(-y))/(np.exp(y) + np.exp(-y))\n",
        "  df = 1 - f*f\n",
        "  return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Al073jXhC1Pv",
        "colab_type": "code",
        "outputId": "c9c673cd-c85a-4e5d-cffe-d9dde5a9b196",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "print(truncate(sigmoid(0),1))\n",
        "print(truncate(dsigmoid(sigmoid(0)),2))\n",
        "print(truncate(tanh(dsigmoid(sigmoid(0))),5))\n",
        "truncate(dtanh(tanh(dsigmoid(sigmoid(0)))),5)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5\n",
            "0.23\n",
            "0.23077\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.94857"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeCvVH1v6Me-",
        "colab_type": "text"
      },
      "source": [
        "# Quiz Question 1\n",
        "\n",
        "What is the value of sigmoid(0) calculated from  your code? (Answer up to 1 decimal point, e.g. 4.2 and NOT 4.29999999, no rounding off).\n",
        "\n",
        "# Quiz Question 2\n",
        "\n",
        "What is the value of dsigmoid(sigmoid(0)) calculated from your code?? (Answer up to 2 decimal point, e.g. 4.29 and NOT 4.29999999, no rounding off). \n",
        "\n",
        "# Quiz Question 3\n",
        "\n",
        "What is the value of tanh(dsigmoid(sigmoid(0))) calculated from your code?? (Answer up to 5 decimal point, e.g. 4.29999 and NOT 4.29999999, no rounding off).\n",
        "\n",
        "# Quiz Question 4\n",
        "\n",
        "What is the value of dtanh(tanh(dsigmoid(sigmoid(0)))) calculated from your code?? (Answer up to 5 decimal point, e.g. 4.29999 and NOT 4.29999999, no rounding off)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EeSVipDu8iKE",
        "colab_type": "text"
      },
      "source": [
        "# Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICbWNemE6LGV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Param:\n",
        "    def __init__(self, name, value):\n",
        "      self.name = name\n",
        "      self.v = value # parameter value\n",
        "      self.d = np.zeros_like(value) # derivative\n",
        "      self.m = np.zeros_like(value) # momentum for Adagrad"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j83pZNPE8212",
        "colab_type": "text"
      },
      "source": [
        "We use random weights with normal distribution (0, weight_sd) for  tanh  activation function and (0.5, weight_sd) for  `sigmoid`  activation function.\n",
        "\n",
        "Biases are initialized to zeros."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swHwLXOI9E7V",
        "colab_type": "text"
      },
      "source": [
        "# LSTM \n",
        "You are making this network, please note f, i, c and o (also \"v\") in the image below:\n",
        "![alt text](http://blog.varunajayasiri.com/ml/lstm.svg)\n",
        "\n",
        "Please note that we are concatenating the old_hidden_vector and new_input."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0DBzNY-90s5",
        "colab_type": "text"
      },
      "source": [
        "# Quiz Question 4\n",
        "\n",
        "In the class definition below, what should be size_a, size_b, and size_c? ONLY use the variables defined above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFuHhqVq6Wge",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "size_a = Hidden_Layer_size # 10 \n",
        "size_b = z_size # X_size + Hidden_Layer_size ( 75 + 10)\n",
        "size_c = X_size # write your code here\n",
        "\n",
        "# Hidden_Layer_size + X_size\n",
        "class Parameters:\n",
        "    def __init__(self):\n",
        "\n",
        "        # Wf matrix is [10 x 85]\n",
        "        self.W_f = Param('W_f', np.random.randn(size_a, size_b) * weight_sd + 0.5)\n",
        "        \n",
        "        # Bias 10 x 1\n",
        "        self.b_f = Param('b_f', np.zeros((size_a, 1)))\n",
        "\n",
        "        self.W_i = Param('W_i', np.random.randn(size_a, size_b) * weight_sd + 0.5)\n",
        "        self.b_i = Param('b_i', np.zeros((size_a, 1)))\n",
        "\n",
        "        self.W_C = Param('W_C', np.random.randn(size_a, size_b) * weight_sd)\n",
        "        self.b_C = Param('b_C', np.zeros((size_a, 1)))\n",
        "\n",
        "        self.W_o = Param('W_o', np.random.randn(size_a, size_b) * weight_sd + 0.5)\n",
        "        self.b_o = Param('b_o', np.zeros((size_a, 1)))\n",
        "\n",
        "        #For final layer to predict the next character\n",
        "        self.W_v = Param('W_v', np.random.randn(X_size, size_a) * weight_sd)\n",
        "        self.b_v = Param('b_v', np.zeros((size_c, 1)))\n",
        "        \n",
        "    def all(self):\n",
        "        return [self.W_f, self.W_i, self.W_C, self.W_o, self.W_v,\n",
        "               self.b_f, self.b_i, self.b_C, self.b_o, self.b_v]\n",
        "        \n",
        "parameters = Parameters()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzmfGLZt_xVs",
        "colab_type": "text"
      },
      "source": [
        "Look at these operations which we'll be writing:\n",
        "\n",
        "**Concatenation of h and x:**\n",
        "\n",
        "$z\\:=\\:\\left[h_{t-1},\\:x\\right]$\n",
        "\n",
        "$f_t=\\sigma\\left(W_f\\cdot z\\:+\\:b_f\\:\\right)$\n",
        "\n",
        "$i_i=\\sigma\\left(W_i\\cdot z\\:+\\:b_i\\right)$\n",
        "\n",
        "$\\overline{C_t}=\\tanh\\left(W_C\\cdot z\\:+\\:b_C\\right)$\n",
        "\n",
        "$C_t=f_t\\ast C_{t-1}+i_t\\ast \\overline{C}_t$\n",
        "\n",
        "$o_t=\\sigma\\left(W_o\\cdot z\\:+\\:b_i\\right)$\n",
        "\n",
        "$h_t=o_t\\ast\\tanh\\left(C_t\\right)$\n",
        "\n",
        "**Logits:**\n",
        "\n",
        "$v_t=W_v\\cdot h_t+b_v$\n",
        "\n",
        "**Softmax:**\n",
        "\n",
        "$\\hat{y}=softmax\\left(v_t\\right)$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bUkseNnDott",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def forward(x, h_prev, C_prev, p = parameters):\n",
        "    assert x.shape == (X_size, 1)\n",
        "    assert h_prev.shape == (Hidden_Layer_size, 1)\n",
        "    assert C_prev.shape == (Hidden_Layer_size, 1)\n",
        "    \n",
        "    #W_f, W_i, W_C, W_o, W_v, b_f, b_i, b_C, b_o, b_v = parameters.all()\n",
        "\n",
        "    z = np.row_stack((h_prev, x))\n",
        "    f = sigmoid(np.dot(parameters.W_f.v, z) + parameters.b_f.v)\n",
        "    i = sigmoid(np.dot(parameters.W_i.v, z) + parameters.b_i.v)\n",
        "    C_bar = tanh(np.dot(parameters.W_C.v, z) + parameters.b_C.v)\n",
        "\n",
        "    C = f*C_prev + i*C_bar\n",
        "    o = sigmoid(np.dot(parameters.W_o.v, z) + parameters.b_o.v)\n",
        "    h = o*tanh(C)\n",
        "\n",
        "    v = np.dot(parameters.W_v.v, h) + parameters.b_v.v\n",
        "\n",
        "    y = np.exp(v) / np.sum(np.exp(v)) #softmax\n",
        "\n",
        "    return z, f, i, C_bar, C, o, h, v, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZrDhZIjFpdI",
        "colab_type": "text"
      },
      "source": [
        "You must finish the function above before you can attempt the questions below. \n",
        "\n",
        "# Quiz Question 5\n",
        "\n",
        "What is the output of 'print(len(forward(np.zeros((X_size, 1)), np.zeros((Hidden_Layer_size, 1)), np.zeros((Hidden_Layer_size, 1)), parameters)))'?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTN8kUAW0KwO",
        "colab_type": "code",
        "outputId": "386264f0-fe04-4c04-b6cc-31a6255e0636",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(len(forward(np.zeros((X_size, 1)), np.zeros((Hidden_Layer_size, 1)), np.zeros((Hidden_Layer_size, 1)), parameters)))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XV-YVl_GGiX8",
        "colab_type": "text"
      },
      "source": [
        "# Quiz Question 6. \n",
        "\n",
        "Assuming you have fixed the forward function, run this command: \n",
        "z, f, i, C_bar, C, o, h, v, y = forward(np.zeros((X_size, 1)), np.zeros((Hidden_Layer_size, 1)), np.zeros((Hidden_Layer_size, 1)))\n",
        "\n",
        "Now, find these values:\n",
        "\n",
        "\n",
        "1.   print(z.shape)\n",
        "2.   print(np.sum(z))\n",
        "3.   print(np.sum(f))\n",
        "\n",
        "Copy and paste exact values you get in the logs into the quiz.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9auEX0zV7zvc",
        "colab_type": "code",
        "outputId": "1b987dc5-ba7d-459b-b6aa-2249a8483bfe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "z, f, i, C_bar, C, o, h, v, y = forward(np.zeros((X_size, 1)), np.zeros((Hidden_Layer_size, 1)), np.zeros((Hidden_Layer_size, 1)))\n",
        "print(z.shape)\n",
        "print(np.sum(z))\n",
        "print(np.sum(z))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(175, 1)\n",
            "0.0\n",
            "0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GvKVWmTDt3H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "z, f, i, C_bar, C, o, h, v, y = forward(np.zeros((X_size, 1)), np.zeros((Hidden_Layer_size, 1)), np.zeros((Hidden_Layer_size, 1)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeSvhkqwILsG",
        "colab_type": "text"
      },
      "source": [
        "# Backpropagation\n",
        "\n",
        "Here we are defining the backpropagation. It's too complicated, here is the whole code. (Please note that this would work only if your earlier code is perfect)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIa1jUZiGPmF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def backward(target, dh_next, dC_next, C_prev,\n",
        "             z, f, i, C_bar, C, o, h, v, y,\n",
        "             p = parameters):\n",
        "    \n",
        "    assert z.shape == (X_size + Hidden_Layer_size, 1)\n",
        "    assert v.shape == (X_size, 1)\n",
        "    assert y.shape == (X_size, 1)\n",
        "    \n",
        "    for param in [dh_next, dC_next, C_prev, f, i, C_bar, C, o, h]:\n",
        "        assert param.shape == (Hidden_Layer_size, 1)\n",
        "        \n",
        "    dv = np.copy(y)\n",
        "    dv[target] -= 1\n",
        "\n",
        "    p.W_v.d += np.dot(dv, h.T)\n",
        "    p.b_v.d += dv\n",
        "\n",
        "    dh = np.dot(p.W_v.v.T, dv)        \n",
        "    dh += dh_next\n",
        "    do = dh * tanh(C)\n",
        "    do = dsigmoid(o) * do\n",
        "    p.W_o.d += np.dot(do, z.T)\n",
        "    p.b_o.d += do\n",
        "\n",
        "    dC = np.copy(dC_next)\n",
        "    dC += dh * o * dtanh(tanh(C))\n",
        "    dC_bar = dC * i\n",
        "    dC_bar = dtanh(C_bar) * dC_bar\n",
        "    p.W_C.d += np.dot(dC_bar, z.T)\n",
        "    p.b_C.d += dC_bar\n",
        "\n",
        "    di = dC * C_bar\n",
        "    di = dsigmoid(i) * di\n",
        "    p.W_i.d += np.dot(di, z.T)\n",
        "    p.b_i.d += di\n",
        "\n",
        "    df = dC * C_prev\n",
        "    df = dsigmoid(f) * df\n",
        "    p.W_f.d += np.dot(df, z.T)\n",
        "    p.b_f.d += df\n",
        "\n",
        "    dz = (np.dot(p.W_f.v.T, df)\n",
        "         + np.dot(p.W_i.v.T, di)\n",
        "         + np.dot(p.W_C.v.T, dC_bar)\n",
        "         + np.dot(p.W_o.v.T, do))\n",
        "    dh_prev = dz[:Hidden_Layer_size, :]\n",
        "    dC_prev = f * dC\n",
        "    \n",
        "    return dh_prev, dC_prev"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tnc7WpRkIU5S",
        "colab_type": "text"
      },
      "source": [
        "# Forward and Backward Combined Pass\n",
        "\n",
        "Let's first clear the gradients before each backward pass"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJWoC3U1ITf8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clear_gradients(params = parameters):\n",
        "    for p in params.all():\n",
        "        p.d.fill(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XN93UnjIgmA",
        "colab_type": "text"
      },
      "source": [
        "Clip gradients to mitigate exploding gradients"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LTsublxIfFl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clip_gradients(params = parameters):\n",
        "    for p in params.all():\n",
        "        np.clip(p.d, -1, 1, out=p.d)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7XUpDTWIl_Y",
        "colab_type": "text"
      },
      "source": [
        "Calculate and store the values in forward pass. Accumulate gradients in backward pass and clip gradients to avoid exploding gradients.\n",
        "\n",
        "input, target are list of integers, with character indexes.\n",
        "h_prev is the array of initial h at  h−1  (size H x 1)\n",
        "C_prev is the array of initial C at  C−1  (size H x 1)\n",
        "Returns loss, final  hT  and  CT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQNxjTuZIia_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def forward_backward(inputs, targets, h_prev, C_prev):\n",
        "    global paramters\n",
        "    \n",
        "    # To store the values for each time step\n",
        "    x_s, z_s, f_s, i_s,  = {}, {}, {}, {}\n",
        "    C_bar_s, C_s, o_s, h_s = {}, {}, {}, {}\n",
        "    v_s, y_s =  {}, {}\n",
        "    \n",
        "    # Values at t - 1\n",
        "    h_s[-1] = np.copy(h_prev)\n",
        "    C_s[-1] = np.copy(C_prev)\n",
        "    \n",
        "    loss = 0\n",
        "    # Loop through time steps\n",
        "    assert len(inputs) == Time_steps\n",
        "    for t in range(len(inputs)):\n",
        "        x_s[t] = np.zeros((X_size, 1))\n",
        "        x_s[t][inputs[t]] = 1 # Input character\n",
        "        \n",
        "        (z_s[t], f_s[t], i_s[t],\n",
        "        C_bar_s[t], C_s[t], o_s[t], h_s[t],\n",
        "        v_s[t], y_s[t]) = \\\n",
        "            forward(x_s[t], h_s[t - 1], C_s[t - 1]) # Forward pass\n",
        "            \n",
        "        loss += -np.log(y_s[t][targets[t], 0]) # Loss for at t\n",
        "        \n",
        "    clear_gradients()\n",
        "\n",
        "    dh_next = np.zeros_like(h_s[0]) #dh from the next character\n",
        "    dC_next = np.zeros_like(C_s[0]) #dh from the next character\n",
        "\n",
        "    for t in reversed(range(len(inputs))):\n",
        "        # Backward pass\n",
        "        dh_next, dC_next = \\\n",
        "            backward(target = targets[t], dh_next = dh_next,\n",
        "                     dC_next = dC_next, C_prev = C_s[t-1],\n",
        "                     z = z_s[t], f = f_s[t], i = i_s[t], C_bar = C_bar_s[t],\n",
        "                     C = C_s[t], o = o_s[t], h = h_s[t], v = v_s[t],\n",
        "                     y = y_s[t])\n",
        "\n",
        "    clip_gradients()\n",
        "        \n",
        "    return loss, h_s[len(inputs) - 1], C_s[len(inputs) - 1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcy5u_vRItkV",
        "colab_type": "text"
      },
      "source": [
        "# Sample the next character"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8SrtJiwIsSm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample(h_prev, C_prev, first_char_idx, sentence_length):\n",
        "    x = np.zeros((X_size, 1))\n",
        "    x[first_char_idx] = 1\n",
        "\n",
        "    h = h_prev\n",
        "    C = C_prev\n",
        "\n",
        "    indexes = []\n",
        "    \n",
        "    for t in range(sentence_length):\n",
        "        _, _, _, _, C, _, h, _, p = forward(x, h, C)\n",
        "        idx = np.random.choice(range(X_size), p=p.ravel())\n",
        "        x = np.zeros((X_size, 1))\n",
        "        x[idx] = 1\n",
        "        indexes.append(idx)\n",
        "\n",
        "    return indexes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiWFaWLNIx_L",
        "colab_type": "text"
      },
      "source": [
        "# Training (Adagrad)\n",
        "\n",
        "Update the graph and display a sample output\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENQYU-7AIw0t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def update_status(inputs, h_prev, C_prev):\n",
        "    #initialized later\n",
        "    global plot_iter, plot_loss\n",
        "    global smooth_loss\n",
        "    \n",
        "    # Get predictions for 200 letters with current model\n",
        "\n",
        "    sample_idx = sample(h_prev, C_prev, inputs[0], 200)\n",
        "    txt = ''.join(idx_to_char[idx] for idx in sample_idx)\n",
        "\n",
        "    # Clear and plot\n",
        "    plt.plot(plot_iter, plot_loss)\n",
        "    display.clear_output(wait=True)\n",
        "    plt.show()\n",
        "\n",
        "    #Print prediction and loss\n",
        "    print(\"----\\n %s \\n----\" % (txt, ))\n",
        "    print(\"iter %d, loss %f\" % (iteration, smooth_loss))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACXcASJuI73a",
        "colab_type": "text"
      },
      "source": [
        "# Update Parameters\n",
        "\n",
        "\\begin{align}\n",
        "\\theta_i &= \\theta_i - \\eta\\frac{d\\theta_i}{\\sum dw_{\\tau}^2} \\\\\n",
        "d\\theta_i &= \\frac{\\partial L}{\\partial \\theta_i}\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bR08TvcjI4Pf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def update_paramters(params = parameters):\n",
        "    for p in params.all():\n",
        "        p.m += p.d * p.d # Calculate sum of gradients\n",
        "        #print(learning_rate * dparam)\n",
        "        p.v += -(learning_rate * p.d / np.sqrt(p.m + 1e-8))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "La9vyJ6RJLFK",
        "colab_type": "text"
      },
      "source": [
        "To delay the keyboard interrupt to prevent the training from stopping in the middle of an iteration\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVDHbMb7JNGT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Exponential average of loss\n",
        "# Initialize to a error of a random model\n",
        "smooth_loss = -np.log(1.0 / X_size) * Time_steps\n",
        "\n",
        "iteration, pointer = 0, 0\n",
        "\n",
        "# For the graph\n",
        "plot_iter = np.zeros((0))\n",
        "plot_loss = np.zeros((0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HF6vS0VWJqsS",
        "colab_type": "text"
      },
      "source": [
        "# Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQyNSL0iJOxH",
        "colab_type": "code",
        "outputId": "90df4d77-6c87-4027-d2d0-2e9bf4ca49be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        }
      },
      "source": [
        "iter = 50000\n",
        "while iter > 0:\n",
        "  # Reset\n",
        "  if pointer + Time_steps >= len(data) or iteration == 0:\n",
        "      g_h_prev = np.zeros((Hidden_Layer_size, 1))\n",
        "      g_C_prev = np.zeros((Hidden_Layer_size, 1))\n",
        "      pointer = 0\n",
        "\n",
        "\n",
        "  inputs = ([char_to_idx[ch] \n",
        "              for ch in data[pointer: pointer + Time_steps]])\n",
        "  targets = ([char_to_idx[ch] \n",
        "              for ch in data[pointer + 1: pointer + Time_steps + 1]])\n",
        "\n",
        "  loss, g_h_prev, g_C_prev = \\\n",
        "      forward_backward(inputs, targets, g_h_prev, g_C_prev)\n",
        "  smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
        "\n",
        "  # Print every hundred steps\n",
        "  if iteration % 100 == 0:\n",
        "      update_status(inputs, g_h_prev, g_C_prev)\n",
        "\n",
        "  update_paramters()\n",
        "\n",
        "  plot_iter = np.append(plot_iter, [iteration])\n",
        "  plot_loss = np.append(plot_loss, [loss])\n",
        "\n",
        "  pointer += Time_steps\n",
        "  iteration += 1\n",
        "  iter = iter -1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXEAAAD1CAYAAACm0cXeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYmIKcRERE/t/NtZvS3Cqrs0AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "----\n",
            " o  N. rhonoft onth gcWe tae in aytg thd AR rmiil ps ysT  LySllaa sabMi.\n",
            "wot e ishaete .\n",
            "th cr  C mki(hiron ie e n n oreg. mi9ibero gro aicslo  Noion f Wede  iavTclwdr rathiaviand atte’an \n",
            "5durWss ty 4 \n",
            "----\n",
            "iter 43800, loss 113.967396\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AKpa1BGOItQ",
        "colab_type": "text"
      },
      "source": [
        "# Quiz Question 7. \n",
        "\n",
        "Run the above code for 50000 iterations making sure that you have 100 hidden layers and time_steps is 40. What is the loss value you're seeing?"
      ]
    }
  ]
}